{"meta":{"title":"Hagrid Xu's Blog","subtitle":null,"description":"just another guy want to be a computer scientist","author":"Hagrid Xu","url":"http://yoursite.com"},"pages":[{"title":"categories","date":"2018-11-27T07:33:48.000Z","updated":"2018-11-27T07:54:21.866Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-11-27T07:36:54.000Z","updated":"2018-11-27T07:54:23.109Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"cs231知识回顾总结","slug":"cs231知识回顾总结","date":"2018-12-11T08:14:41.000Z","updated":"2018-12-11T09:48:37.674Z","comments":true,"path":"2018/12/11/cs231知识回顾总结/","link":"","permalink":"http://yoursite.com/2018/12/11/cs231知识回顾总结/","excerpt":"","text":"图像分类流程 输入：输入是包含N个图像的集合，每个图像的标签是K种分类标签中的一种。这个集合称为训练集。 学习：这一步的任务是使用训练集来学习每个类到底长什么样。一般该步骤叫做训练分类器或者学习一个模型。 评价：让分类器来预测它未曾见过的图像的分类标签，并以此来评价分类器的质量。我们会把分类器预测的标签和图像真正的分类标签对比。毫无疑问，分类器预测的分类标签和图像真正的分类标签如果一致，那就是好事，这样的情况越多越好。 Nearest Neighbor分类器 L1距离 L2距离 k-Nearest Neighbor分类器 验证集 超参数调优（尝试不同的K值，不同的距离计算方法） 交叉验证（如果训练数据量不够，使用交叉验证方法，它能帮助我们在选取最优超参数的时候减少噪音） 线性分类简单来说平面空间中有点，划一条线。（与圈一个圈有区别） 基于参数的评分函数。该函数将原始图像像素映射为分类评分值（例如：一个线性函数）。 损失函数。该函数能够根据分类评分和训练集图像数据实际分类的一致性，衡量某个具体参数集的质量好坏。损失函数有多种版本和不同的实现方式（例如：Softmax或SVM）。 评分函数（参数parameters、权重weights、偏差向量bias vector） 偏差与权重的合并（矩阵计算的合并） 归一处理（预处理） 与kNN分类器不同，参数方法的优势在于一旦通过训练学习到了参数，就可以将训练数据丢弃了。同时该方法对于新的测试数据的预测非常快，因为只需要与权重W进行一个矩阵乘法运算。 损失函数 SVM 正则化 softmax分类器 针对一个数据点，SVM和Softmax分类器的不同处理方式的例子。两个分类器都计算了同样的分值向量f（本节中是通过矩阵乘来实现）。不同之处在于对f中分值的解释：SVM分类器将它们看做是分类评分，它的损失函数鼓励正确的分类（本例中是蓝色的类别2）的分值比其他分类的分值高出至少一个边界值。Softmax分类器将这些数值看做是每个分类没有归一化的对数概率，鼓励正确分类的归一化的对数概率变高，其余的变低。SVM的最终的损失值是1.58，Softmax的最终的损失值是0.452，但要注意这两个数值没有可比性。只在给定同样数据，在同样的分类器的损失值计算中，它们才有意义。 Softmax分类器为每个分类提供了“可能性”：SVM的计算是无标定的，而且难以针对所有分类的评分值给出直观解释。Softmax分类器则不同，它允许我们计算出对于所有分类标签的可能性。举个例子，针对给出的图像，SVM分类器可能给你的是一个[12.5, 0.6, -23.0]对应分类“猫”，“狗”，“船”。而softmax分类器可以计算出这三个标签的”可能性“是[0.9, 0.09, 0.01]，这就让你能看出对于不同分类准确性的把握。 最优化 Optimization损失函数可以量化某个具体权重集W的质量。而最优化的目标就是找到能够最小化损失函数值的W 随机梯度下降 梯度计算计算梯度有两种方法：一个是缓慢的近似方法（数值梯度法），但实现相对简单。另一个方法（分析梯度法）计算迅速，结果精确，但是实现时容易出错，且需要使用微分。 使用有限差值近似计算梯度比较简单，但缺点在于终究只是近似（因为我们对于h值是选取了一个很小的数值，但真正的梯度定义中h趋向0的极限），且耗费计算资源太多。第二个梯度计算方法是利用微分来分析，能得到计算梯度的公式（不是近似），用公式计算梯度速度很快，唯一不好的就是实现的时候容易出错。为了解决这个问题，在实际操作时常常将分析梯度法的结果和数值梯度法的结果作比较，以此来检查其实现的正确性，这个步骤叫做梯度检查。 https://zhuanlan.zhihu.com/p/21387326?refer=intelligentunit 反向传播反向传播是利用链式法则递归计算表达式的梯度的方法。理解反向传播过程及其精妙之处，对于理解、实现、设计和调试神经网络非常关键。 https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit 神经网络常用基础函数 sigmoid tanh ReLU leaky ReLU Maxout 灵活地组织层 fully connect layer全连接层 输出层 隐藏层 数据预处理 均值减法 归一 PCA和白化whitening 权重初始化：用小随机数来初始化，而不是置0 偏置初始化：一般置0 批量归一化（Batch Normalization）让激活数据在训练开始前通过一个网络，网络处理数据使其服从标准高斯分布。在实现层面，应用这个技巧通常意味着全连接层（或者是卷积层，后续会讲）与激活函数之间添加一个BatchNorm层。 正则化 RegularizationL2正则化 L1正则化 最大范式约束 随机失活 https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit 梯度检查中心化公式+相对误差 使用双精度浮点数 保持浮点数的有效范围 目标函数的不可导点 检查时关闭dropout与augmentation https://zhuanlan.zhihu.com/p/21741716?refer=intelligentunit https://zhuanlan.zhihu.com/p/21798784?refer=intelligentunit 卷积神经网络卷积层 汇聚层 归一化层 全连接层 全连接层转化成卷积层 https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit","categories":[],"tags":[]},{"title":"2018.11.01-09学习知识列表","slug":"2018-11-01-09学习知识列表","date":"2018-11-09T05:56:32.000Z","updated":"2018-11-27T07:38:50.348Z","comments":true,"path":"2018/11/09/2018-11-01-09学习知识列表/","link":"","permalink":"http://yoursite.com/2018/11/09/2018-11-01-09学习知识列表/","excerpt":"","text":"11.1 组会讨论 11.2 cs231n：神经网络笔记：Nesterov动量更新方法，学习率衰减方法，超参数调优，迁移学习 11.5 自然语言处理：语料库与语言知识库 11.6 自然语言处理：语言模型章节的各种平滑方法 nature DL 综述 latex写作业 周五课程ppt 11.7 自然语言处理：推导语言模型章节的公式 cs61a：week1课程 11.8 周会报告 深度学习与知识图谱5.1–5.2 python leetcode 11.9 自然语言处理：概率图模型（贝叶斯模型、马尔可夫模型） 回顾一个月的学习内容","categories":[{"name":"学习知识列表","slug":"学习知识列表","permalink":"http://yoursite.com/categories/学习知识列表/"}],"tags":[{"name":"学习知识列表","slug":"学习知识列表","permalink":"http://yoursite.com/tags/学习知识列表/"}]},{"title":"2018.10学习知识列表","slug":"2018-10学习知识列表","date":"2018-11-09T05:40:01.000Z","updated":"2018-11-27T07:38:51.712Z","comments":true,"path":"2018/11/09/2018-10学习知识列表/","link":"","permalink":"http://yoursite.com/2018/11/09/2018-10学习知识列表/","excerpt":"","text":"week1 知识图谱发展报告 中文信息发展报告 week2 cs231n 第一、二周课程与笔记：发展、目标识别、图像分类、线性分类 机器学习书籍：模型评估和选择 cs231n 第三周课程：损失函数与优化SVM、Softmax、最优化方法的随机搜索、随机局部搜索、跟随梯度搜索 python 与 numpy tutorial cs231n偏差和权重、PCA latex安装学习 cs231n第4周课程：反向传播、梯度计算 week3 神经概率语言模型论文：n-gram、词向量，神经网络词序与梯度下降优化、异步实现 jupyter 人工智能ppt 数学之美：自然语言处理传统方法，中文分词，隐马尔可夫模型 week4 cs231n：卷积与池化 数学之美：上下文相关判定歧义、搜索引擎排名、有限状态机、最大熵迭代算法、余弦定理相似度、TF-IDF R-SVM+：Robust Learning with privileged information cs231n：Relu激活函数、正则化解决过拟合问题，学习率设置防止神经元死亡 cncc大会 cs231n：数据预处理、权重初始化，assignment1 KNN实现 学习numpy官方使用文档 Week5（29，30，31） 统计自然语言处理 交叉验证 批量归一化，BatchNorm SVM zsh leetcode","categories":[{"name":"学习知识列表","slug":"学习知识列表","permalink":"http://yoursite.com/categories/学习知识列表/"}],"tags":[{"name":"学习知识列表","slug":"学习知识列表","permalink":"http://yoursite.com/tags/学习知识列表/"}]}]}